{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Figures 5-6\n",
    "\n",
    "These figures show clustering metrics and PCA\n",
    "\n",
    "For Sawi et al., 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from obspy import read\n",
    "from matplotlib import cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import datetime as dtt\n",
    "import matplotlib.patches\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from matplotlib.patches import Rectangle\n",
    "import sklearn.metrics\n",
    "from scipy import spatial\n",
    "import matplotlib.image as mpimg\n",
    "import obspy\n",
    "from scipy.signal import butter, lfilter\n",
    "import librosa\n",
    "# sys.path.insert(0, '../01_DataPrep')\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as spio\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import scipy.io as spio\n",
    "import scipy.signal\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('../src/visualization/')\n",
    "\n",
    "import paths\n",
    "from sklearn.cluster import KMeans\n",
    "# import figureFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from functions2 import getFeatures, getLocationFeatures,getNMFOrder,resortByNMF,getSpectra_fromWF,getSgram\n",
    "from functions2 import PCAonFP,calcSilhScore,getDailyTempDiff,getSpectraMedian,CalcDiffPeak,PVEofPCA,getTopFCat \n",
    "from functions2 import calcFFT, getWF, swapLabels,trimSpectra, KMeansSpectra, compileSpectraFromWF\n",
    "import figureFunctions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions (move later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateToEventID(cat):\n",
    "    \n",
    "    evID = []\n",
    "    \n",
    "    for i, dt in enumerate(cat.datetime):\n",
    "        \n",
    "        a = str(dt)    \n",
    "        b = a.replace('-','').replace(':','').replace(' ','')[3:]\n",
    "        \n",
    "        \n",
    "        evID.append(b)\n",
    "        \n",
    "    cat['event_ID'] = evID\n",
    "    \n",
    "    return cat\n",
    "\n",
    "\n",
    "\n",
    "def getDailyTempDiff2(garciaDF_H,garciaDF_D,**plt_kwargs):\n",
    "\n",
    "    tstart      =     plt_kwargs['tstartreal']\n",
    "    tend        =     plt_kwargs['tendreal']\n",
    "\n",
    "    garciaDF_H1 = garciaDF_H[garciaDF_H.datetime>=tstart]\n",
    "    garciaDF_H1 = garciaDF_H1[garciaDF_H1.datetime<tend]\n",
    "\n",
    "    garciaDF_D1 = garciaDF_D[garciaDF_D.datetime>=tstart]\n",
    "    garciaDF_D1 = garciaDF_D1[garciaDF_D1.datetime<tend]\n",
    "\n",
    "\n",
    "    temp_H = garciaDF_H1.temp_H.bfill()\n",
    "    temp_H_a = np.array(temp_H)\n",
    "\n",
    "    temp_H_a_r = temp_H_a.reshape(len(garciaDF_D1),24)\n",
    "    mean_diff = []\n",
    "    for i in range(len(temp_H_a_r[:,0])):\n",
    "    #     plt.plot(temp_H_a_r[i,:] - garciaDF_D1.temp_D.iloc[i])\n",
    "        mean_diff.append(temp_H_a_r[i,:] - garciaDF_D1.temp_D.iloc[i])\n",
    "\n",
    "\n",
    "    mean_mean_diff = np.mean(mean_diff,axis=0)\n",
    "    return mean_mean_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load project variables: names and paths\n",
    "# key = sys.argv[1]\n",
    "\n",
    "key = \"BB_Gorner_Event_Final_v11_J8\"\n",
    "keyN = \"BB_Gorner_Cont_Final_v10_J8\"\n",
    "\n",
    "\n",
    "filetype = '.gse2'     \n",
    "filetypeN = '.sac' \n",
    "\n",
    "\n",
    "p = paths.returnp(key)\n",
    "pN = paths.returnp(keyN)\n",
    "\n",
    "#%%\n",
    "\n",
    "projName        = p['projName']\n",
    "datasetID       = p['datasetID']\n",
    "projName        = p['projName']\n",
    "station         = p['station']\n",
    "channel         = p['channel']\n",
    "path_top        = p['path_top']\n",
    "path_proj       = p['path_proj']\n",
    "outfile_name    = p['outfile_name']\n",
    "dataFile_name   = p['dataFile_name']\n",
    "path_WF         = p['path_WF']\n",
    "path_Cat        = p['path_Cat'] #original, raw catalog\n",
    "subCatalog_Name = f\"{dataFile_name}_Sgrams_Subcatalog.hdf5\"\n",
    "\n",
    "\n",
    "\n",
    "pathFP          = f'{path_top}{projName}/03_output/{station}/SpecUFEx_output/step4_FEATout/'\n",
    "pathACM         = f'{path_top}{projName}/03_output/{station}/SpecUFEx_output/step2_NMF/'\n",
    "pathSTM         = f'{path_top}{projName}/03_output/{station}/SpecUFEx_output/step4_stateTransMats/'\n",
    "pathEB          = f'{path_top}{projName}/02_src/02_SpecUFEx/EB.mat'\n",
    "pathElnB          = f'{path_top}{projName}/02_src/02_SpecUFEx/ElnB.mat'\n",
    "pathW        = path_proj + '02_src/02_SpecUFEx/out.DictGain.mat' \n",
    "\n",
    "\n",
    "# pathClusCat = path_proj + f\"principalDf_full_{mode}_Kopt{Kopt}.csv\"\n",
    "dataH5_path = path_proj + dataFile_name\n",
    "\n",
    "\n",
    "projNameN        = pN['projName']\n",
    "datasetIDN       = pN['datasetID']\n",
    "projNameN        = pN['projName']\n",
    "station         = pN['station']\n",
    "channel         = pN['channel']\n",
    "\n",
    "\n",
    "path_top        = pN['path_top']\n",
    "path_projN       = pN['path_proj']\n",
    "outfile_nameN    = pN['outfile_name']\n",
    "dataFile_nameN   = pN['dataFile_name']\n",
    "path_WFN         = pN['path_WF']\n",
    "path_CatN        = pN['path_Cat'] #original, raw catalog\n",
    "subCatalog_NameN = f\"{dataFile_name}_Sgrams_Subcatalog.hdf5\"\n",
    "\n",
    "\n",
    "pathACMN         = f'{path_top}{projNameN}/03_output/{station}/SpecUFEx_output/step2_NMF/'\n",
    "pathSTMN         = f'{path_top}{projNameN}/03_output/{station}/SpecUFEx_output/step4_stateTransMats/'\n",
    "pathEBN          = f'{path_top}{projNameN}/02_src/02_SpecUFEx/EB.mat'\n",
    "pathElnBN          = f'{path_top}{projNameN}/02_src/02_SpecUFEx/ElnB.mat'\n",
    "pathWN        = path_projN + '02_src/02_SpecUFEx/out.DictGain.mat' \n",
    "\n",
    "\n",
    "\n",
    "# pathClusCatN = path_projN + f\"principalDf_full_{mode}_Kopt{KoptN}.csv\"\n",
    "dataH5_pathN = path_projN + dataFile_nameN\n",
    "\n",
    "\n",
    "pathFig = '../reports/figures/'\n",
    "pathAuxData = '../data/external/GarciaEtAl_2019/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load auxiliary catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lake_D</th>\n",
       "      <th>temp_D</th>\n",
       "      <th>rain_D</th>\n",
       "      <th>gps24_D</th>\n",
       "      <th>gps34_D</th>\n",
       "      <th>gps36_D</th>\n",
       "      <th>gps37_D</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-06-14</th>\n",
       "      <td>25.738000</td>\n",
       "      <td>3.526000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2007-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-15</th>\n",
       "      <td>26.140615</td>\n",
       "      <td>7.312125</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>2007-06-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   lake_D    temp_D    rain_D   gps24_D   gps34_D   gps36_D  \\\n",
       "datetime_index                                                                \n",
       "2007-06-14      25.738000  3.526000  0.500000  0.000000  0.000000  0.000000   \n",
       "2007-06-15      26.140615  7.312125  0.054167  0.033048  0.016641  0.010967   \n",
       "\n",
       "                 gps37_D   datetime  \n",
       "datetime_index                       \n",
       "2007-06-14      0.000000 2007-06-14  \n",
       "2007-06-15      0.012695 2007-06-15  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garciaDF_H = pd.read_csv(f'{pathAuxData}garciaDF_H.csv',index_col=0)\n",
    "garciaDF_3H = pd.read_csv(f'{pathAuxData}garciaDF_3H.csv',index_col=0)\n",
    "garciaDF_D = pd.read_csv(f'{pathAuxData}garciaDF_D.csv',index_col=0)\n",
    "\n",
    "\n",
    "## when loading csv or text, sometimes need to reconvert this column to pandas datetime\n",
    "garciaDF_H['datetime'] = [pd.to_datetime(ii) for ii in garciaDF_H.index]\n",
    "garciaDF_3H['datetime'] = [pd.to_datetime(ii) for ii in garciaDF_3H.index]\n",
    "garciaDF_D['datetime'] = [pd.to_datetime(ii) for ii in garciaDF_D.index]\n",
    "\n",
    "garciaDF_H['datetime_index'] = [pd.to_datetime(ii) for ii in garciaDF_H.index]\n",
    "garciaDF_3H['datetime_index'] = [pd.to_datetime(ii) for ii in garciaDF_3H.index]\n",
    "garciaDF_D['datetime_index'] = [pd.to_datetime(ii) for ii in garciaDF_D.index]\n",
    "\n",
    "\n",
    "garciaDF_H = garciaDF_H.set_index('datetime_index')\n",
    "garciaDF_3H = garciaDF_3H.set_index('datetime_index')\n",
    "garciaDF_D = garciaDF_D.set_index('datetime_index')\n",
    "\n",
    "\n",
    "\n",
    "garciaDF_D.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some important times in study period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing of lake events \n",
    "tstart = dtt.datetime(2007, 6, 13)\n",
    "tend = dtt.datetime(2007, 7, 23)\n",
    "calvet = dtt.datetime(2007, 7, 1,13,41,35)\n",
    "supraDraint = dtt.datetime(2007, 7, 4)\n",
    "subDraint = dtt.datetime(2007, 7, 7)\n",
    "drainEndt = dtt.datetime(2007, 7, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateToEventID(cat):\n",
    "    \n",
    "    evID = []\n",
    "    \n",
    "    for i, dt in enumerate(cat.datetime):\n",
    "        \n",
    "        a = str(dt)    \n",
    "        b = a.replace('-','').replace(':','').replace(' ','')[3:]\n",
    "        \n",
    "        \n",
    "        evID.append(b)\n",
    "        \n",
    "    cat['event_ID'] = evID\n",
    "    \n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "##icequakes\n",
    "\n",
    "cat_raw = pd.read_csv(path_Cat)\n",
    "cat_raw['event_ID'] = [str(int(evv)) for evv in cat_raw['event_ID']]\n",
    "\n",
    "\n",
    "cat_raw['datetime'] = [pd.to_datetime(i) for i in cat_raw.datetime]\n",
    "cat_raw['datetime_index']= [pd.to_datetime(i) for i in cat_raw.datetime]\n",
    "cat_raw = cat_raw.set_index('datetime_index')\n",
    "\n",
    "cat_raw.head(2)\n",
    "\n",
    "\n",
    "#fix event IDs??\n",
    "# cat00 = dateToEventID(cat_raw)\n",
    "cat00 = cat_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "## noise\n",
    "\n",
    "cat_rawN = pd.read_csv(path_CatN)\n",
    "cat_rawN['event_ID'] = [str(int(evv)) for evv in cat_rawN['event_ID']]\n",
    "\n",
    "\n",
    "cat_rawN['datetime'] = [pd.to_datetime(i) for i in cat_rawN.datetime]\n",
    "cat_rawN['datetime_index']= [pd.to_datetime(i) for i in cat_rawN.datetime]\n",
    "cat_rawN = cat_rawN.set_index('datetime_index')\n",
    "\n",
    "cat_rawN.head(2)\n",
    "\n",
    "\n",
    "#fix event IDs??\n",
    "# cat00N = dateToEventID(cat_rawN)\n",
    "cat00N = cat_rawN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "##station data \n",
    "stn = pd.read_csv(\"../data/raw/stnlst.csv\",\n",
    "                  header=None,\n",
    "                  names=['name','X','Y','Elevation','dX','dY','Depth'])\n",
    "\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experiment parameters from H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "####IQIQIQIQIQIQIQIQI\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(path_proj + dataFile_name,'r') as dataFile:\n",
    "\n",
    "    lenData = dataFile['processing_info/'].get('lenData')[()]\n",
    "    fs = dataFile['spec_parameters/'].get('fs')[()]\n",
    "    \n",
    "    # fmin = \n",
    "    nperseg = dataFile['spec_parameters/'].get('nperseg')[()]\n",
    "    noverlap = dataFile['spec_parameters/'].get('noverlap')[()]\n",
    "    nfft = dataFile['spec_parameters/'].get('nfft')[()]\n",
    "\n",
    "\n",
    "    fmax = dataFile['spec_parameters/'].get('fmax')[()]\n",
    "    fmax = np.ceil(fmax)\n",
    "    fmin = dataFile['spec_parameters/'].get('fmin')[()]\n",
    "    fmin = np.floor(fmin)    \n",
    "    fSTFT = dataFile['spec_parameters/'].get('fSTFT')[()]\n",
    "    tSTFT = dataFile['spec_parameters/'].get('tSTFT')[()]\n",
    "    \n",
    "    sgram_mode = dataFile['spec_parameters/'].get('mode')[()].decode('utf-8')\n",
    "    scaling = dataFile['spec_parameters/'].get('scaling')[()].decode('utf-8')\n",
    "    \n",
    "    \n",
    "fs = int(np.ceil(fs))\n",
    "winLen_Sec = float(nperseg / fs)\n",
    "\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "##### NOISENOISENOISENOISENOISE\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "\n",
    "with h5py.File(path_projN + dataFile_nameN,'r') as dataFile:\n",
    "\n",
    "    lenDataN = dataFile['processing_info/'].get('lenData')[()]\n",
    "    fsN = dataFile['spec_parameters/'].get('fs')[()]\n",
    "    \n",
    "    # fminN = \n",
    "    npersegN = dataFile['spec_parameters/'].get('nperseg')[()]\n",
    "    noverlapN = dataFile['spec_parameters/'].get('noverlap')[()]\n",
    "    nfftN = dataFile['spec_parameters/'].get('nfft')[()]\n",
    "\n",
    "\n",
    "    fmaxN = dataFile['spec_parameters/'].get('fmax')[()]\n",
    "    fmaxN = np.ceil(fmaxN)\n",
    "    fminN = dataFile['spec_parameters/'].get('fmin')[()]\n",
    "    fminN = np.floor(fminN)    \n",
    "    fSTFTN = dataFile['spec_parameters/'].get('fSTFT')[()]\n",
    "    tSTFTN = dataFile['spec_parameters/'].get('tSTFT')[()]\n",
    "    \n",
    "    sgram_modeN = dataFile['spec_parameters/'].get('mode')[()].decode('utf-8')\n",
    "    scalingN = dataFile['spec_parameters/'].get('scaling')[()].decode('utf-8')\n",
    "    \n",
    "    \n",
    "fsN = int(np.ceil(fsN))\n",
    "winLen_SecN = float(npersegN / fsN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load specufex output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "## specufex output - IQIQIQIQIQIQIQIQIQIQ\n",
    " \n",
    "#########       #########       #########       #########       #########       #########       #########       #########       Wmat = loadmat(pathW)\n",
    "\n",
    "\n",
    "EBmat = loadmat(pathEB)\n",
    "\n",
    "W = Wmat.get('W1')\n",
    "EB = EBmat.get('EB')\n",
    "\n",
    "\n",
    "\n",
    "numPatterns = len(W[1])\n",
    "Nfreqs = len(W)\n",
    "numStates = EB.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "order_swap = getNMFOrder(W,numPatterns)\n",
    "W_new = resortByNMF(W,order_swap)\n",
    "EB_new = resortByNMF(EB,order_swap)\n",
    "\n",
    "RMM = W_new @ EB_new.T\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "## specufex output - NOISENOISENOINSENOISE\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "\n",
    "WmatN = loadmat(pathWN)\n",
    "EBmatN = loadmat(pathEBN)\n",
    "\n",
    "WN = WmatN.get('W1')\n",
    "EBN = EBmatN.get('EB')\n",
    "\n",
    "\n",
    "\n",
    "numPatternsN = len(WN[1])\n",
    "NfreqsN = len(WN)\n",
    "numStatesN = EBN.shape[0]\n",
    "\n",
    "\n",
    "order_swapN = getNMFOrder(WN,numPatternsN)\n",
    "W_newN = resortByNMF(WN,order_swapN)\n",
    "EB_newN = resortByNMF(EBN,order_swapN)\n",
    "\n",
    "RMMN = W_newN @ EB_newN.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format day ticks, time plotting\n",
    "\n",
    "* Central European Time is 2 hours later than UTC (Coordinated Universal Time) \n",
    "* Max temp occurs around 16:00 (4pm) local time or, 14:00 (2pm) UTC\n",
    "* All times in UTC\n",
    "\n",
    "\n",
    "todo: fix ::\n",
    "\n",
    "\n",
    "##dummy variable -- just needed to get complete day set -- FIXFIX\n",
    "clus_clu_perday = cat0.event_ID.resample('D', label='left', closed='right').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6-14', '6-21', '6-28', '7-5', '7-12', '7-19']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "            ##### FORMAT DAY TICKS (ASSUMES NO DAYS SKIPPED?) ######\n",
    "############################################################\n",
    "tstart = pd.to_datetime('2007-06-14 00:00:00')\n",
    "tend   = pd.to_datetime('2007-07-22 00:00:00')\n",
    "\n",
    "\n",
    "\n",
    "delta_day = 7\n",
    "\n",
    "##dummy variable -- just needed to get complete day set -- FIXFIX\n",
    "clus_clu_perday = cat0.event_ID.resample('D', label='left', closed='right').count()\n",
    "\n",
    "numDays = len(clus_clu_perday)\n",
    "\n",
    "days_list = [clus_clu_perday.index[i] for i in range(numDays)]\n",
    "\n",
    "\n",
    "## these have lots of possible text formats\n",
    "day_labels = [f\"{days_list[d].month}-{days_list[d].date().day}\" for d in range(0,len(days_list),delta_day)]\n",
    "\n",
    "day_ticks = [days_list[d] for d in range(0,len(days_list),delta_day)]\n",
    "\n",
    "\n",
    "# Central European Time is 2 hours later than UTC (Coordinated Universal Time)\n",
    "##max temp is around 4pm local time or 16:00, in UTC it is 14:00 or 2pm\n",
    "#all times in UTC\n",
    "hour_of_approx_max_temp = 14\n",
    "hourMaxTemp = [dtt.datetime(2007, 6, 14,hour_of_approx_max_temp,0,0) + pd.DateOffset(i) for i in range(0,numDays)]\n",
    "\n",
    "hour24labels = [str(r) for r in range(0,24)] #UTC\n",
    "\n",
    "print(day_labels)\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap']='magma'\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "colors =cm.Paired(np.array([1,5,7,9,2,4,6,8]))\n",
    "\n",
    "\n",
    "## when plotting, add a bit of buffer so bars aren't cut off\n",
    "tlimstart = pd.to_datetime('2007-06-13 12:00:00')\n",
    "tlimend   = pd.to_datetime('2007-07-22 12:00:00')\n",
    "\n",
    "\n",
    "lw1=4        \n",
    "lw2=5\n",
    "alphaT=1\n",
    "ylabfont=8\n",
    "ylabpad =10\n",
    "\n",
    "\n",
    "plt_kwargs = {'lw1':lw1,\n",
    "              'lw2':lw2,\n",
    "              'alphaT':alphaT,\n",
    "              'ylabfont':ylabfont,\n",
    "              'ylabpad':ylabpad,\n",
    "              'colors':colors,\n",
    "              'scaling':scaling,\n",
    "              'sgram_mode':sgram_mode,\n",
    "              'hour24labels':hour24labels,\n",
    "              'day_ticks':day_ticks,\n",
    "              'day_labels':day_labels,\n",
    "              'numDays':numDays,\n",
    "              'hourMaxTemp':hourMaxTemp,\n",
    "              'tstart':tlimstart, ## for extending x axis to fit bars\n",
    "              'tend':tlimend,     ## for extending x axis to fit bars\n",
    "              'tstartreal':tstart,## actual study bound\n",
    "              'tendreal':tend     ## actual study bound\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specs for figures JGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quarter page\n",
    "width1 = 3.74016\n",
    "height1 = 4.52756\n",
    "\n",
    "#full page\n",
    "width2 = 7.48031\n",
    "height2 = 9.05512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## move to analysis\n",
    "# clus_sel = [1,2,3]\n",
    "# sel_state = [12,14,8]\n",
    "\n",
    "# clus_selN = [1,2,3,4]\n",
    "# sel_stateN = [6,3,5,4]\n",
    "\n",
    "# leg = ['During/after flood','Before flood','PM']\n",
    "# legN = ['During/after flood','Before flood','Rain?','PM']\n",
    "\n",
    "# ## move to clustering\n",
    "# mode = 'fingerprints'#'kernalPCA'#'fingerprints'#'PCA' \n",
    "\n",
    "# print(key,mode,Kopt, ' clusters')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
