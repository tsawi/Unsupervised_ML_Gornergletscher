{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Figures 5-6\n",
    "\n",
    "These figures show clustering metrics and PCA\n",
    "\n",
    "For Sawi et al., 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from obspy import read\n",
    "from matplotlib import cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import datetime as dtt\n",
    "import matplotlib.patches\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "from  sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "from matplotlib.patches import Rectangle\n",
    "import sklearn.metrics\n",
    "from scipy import spatial\n",
    "import matplotlib.image as mpimg\n",
    "import obspy\n",
    "from scipy.signal import butter, lfilter\n",
    "import librosa\n",
    "# sys.path.insert(0, '../01_DataPrep')\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as spio\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import scipy.io as spio\n",
    "import scipy.signal\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('../src/visualization/')\n",
    "\n",
    "import paths\n",
    "from sklearn.cluster import KMeans\n",
    "# import figureFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from functions2 import getFeatures, getLocationFeatures,getNMFOrder,resortByNMF,getSpectra_fromWF,getSgram\n",
    "from functions2 import PCAonFP,calcSilhScore,getDailyTempDiff,getSpectraMedian,CalcDiffPeak,PVEofPCA,getTopFCat \n",
    "from functions2 import calcFFT, getWF, swapLabels,trimSpectra, KMeansSpectra, compileSpectraFromWF\n",
    "import figureFunctions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions (move later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dateToEventID(cat):\n",
    "    \n",
    "    evID = []\n",
    "    \n",
    "    for i, dt in enumerate(cat.datetime):\n",
    "        \n",
    "        a = str(dt)    \n",
    "        b = a.replace('-','').replace(':','').replace(' ','')[3:]\n",
    "        \n",
    "        \n",
    "        evID.append(b)\n",
    "        \n",
    "    cat['event_ID'] = evID\n",
    "    \n",
    "    return cat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getDailyTempDiff2(garciaDF_H,garciaDF_D,**plt_kwargs):\n",
    "\n",
    "    tstart      =     plt_kwargs['tstartreal']\n",
    "    tend        =     plt_kwargs['tendreal']\n",
    "\n",
    "    garciaDF_H1 = garciaDF_H[garciaDF_H.datetime>=tstart]\n",
    "    garciaDF_H1 = garciaDF_H1[garciaDF_H1.datetime<tend]\n",
    "\n",
    "    garciaDF_D1 = garciaDF_D[garciaDF_D.datetime>=tstart]\n",
    "    garciaDF_D1 = garciaDF_D1[garciaDF_D1.datetime<tend]\n",
    "\n",
    "\n",
    "    temp_H = garciaDF_H1.temp_H.bfill()\n",
    "    temp_H_a = np.array(temp_H)\n",
    "\n",
    "    temp_H_a_r = temp_H_a.reshape(len(garciaDF_D1),24)\n",
    "    mean_diff = []\n",
    "    for i in range(len(temp_H_a_r[:,0])):\n",
    "    #     plt.plot(temp_H_a_r[i,:] - garciaDF_D1.temp_D.iloc[i])\n",
    "        mean_diff.append(temp_H_a_r[i,:] - garciaDF_D1.temp_D.iloc[i])\n",
    "\n",
    "\n",
    "    mean_mean_diff = np.mean(mean_diff,axis=0)\n",
    "    return mean_mean_diff\n",
    "\n",
    "\n",
    "\n",
    "def catMergeFromH5(path_Cat,path_proj,outfile_name):\n",
    "    '''\n",
    "    Keep csv catalog events based on H5 used in SpecUFEx \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## read 'raw' catalog, the immutable one\n",
    "    cat_raw = pd.read_csv(path_Cat)\n",
    "    cat_raw['event_ID'] = [str(int(evv)) for evv in cat_raw['event_ID']]    \n",
    "    \n",
    "    \n",
    "    ## load event IDs from H5\n",
    "    MLout =  h5py.File(path_proj + outfile_name,'r')\n",
    "    evID_kept = [evID.decode('utf-8') for evID in MLout['catalog/event_ID/'][:]]\n",
    "    MLout.close()\n",
    "    \n",
    "    ## put H5 events into pandas dataframe\n",
    "    df_kept = pd.DataFrame({'event_ID':evID_kept})\n",
    "\n",
    "    ## merge based on event ID\n",
    "    cat00 = pd.merge(cat_raw,df_kept,on='event_ID')\n",
    "    \n",
    "    ## if length of H5 events and merged catalog are equal, then success\n",
    "    if len(evID_kept) == len(cat00):\n",
    "        print(f'{len(cat00)} events kept, merge sucessful')\n",
    "    else:\n",
    "        print('check merge -- error may have occurred ')\n",
    "    \n",
    "    \n",
    "    ## convert to datetime, set as index\n",
    "    cat00['datetime'] = [pd.to_datetime(i) for i in cat00.datetime]\n",
    "    cat00['datetime_index']= [pd.to_datetime(i) for i in cat00.datetime]\n",
    "    cat00 = cat00.set_index('datetime_index')    \n",
    "\n",
    "\n",
    "    return cat00\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% load project variables: names and paths\n",
    "# key = sys.argv[1]\n",
    "\n",
    "key = \"BB_Gorner_Event_Final_v11_J8\"\n",
    "keyN = \"BB_Gorner_Cont_Final_v10_J8\"\n",
    "\n",
    "\n",
    "filetype = '.gse2'     \n",
    "filetypeN = '.sac' \n",
    "\n",
    "\n",
    "p = paths.returnp(key)\n",
    "pN = paths.returnp(keyN)\n",
    "\n",
    "#%%\n",
    "\n",
    "projName        = p['projName']\n",
    "datasetID       = p['datasetID']\n",
    "projName        = p['projName']\n",
    "station         = p['station']\n",
    "channel         = p['channel']\n",
    "path_top        = p['path_top']\n",
    "path_proj       = p['path_proj']\n",
    "outfile_name    = p['outfile_name']\n",
    "dataFile_name   = p['dataFile_name']\n",
    "path_WF         = p['path_WF']\n",
    "path_Cat        = p['path_Cat'] #original, raw catalog\n",
    "subCatalog_Name = f\"{dataFile_name}_Sgrams_Subcatalog.hdf5\"\n",
    "\n",
    "\n",
    "\n",
    "pathFP          = f'{path_top}{projName}/03_output/{station}/SpecUFEx_output/step4_FEATout/'\n",
    "pathACM         = f'{path_top}{projName}/03_output/{station}/SpecUFEx_output/step2_NMF/'\n",
    "pathSTM         = f'{path_top}{projName}/03_output/{station}/SpecUFEx_output/step4_stateTransMats/'\n",
    "pathEB          = f'{path_top}{projName}/02_src/02_SpecUFEx/EB.mat'\n",
    "pathElnB          = f'{path_top}{projName}/02_src/02_SpecUFEx/ElnB.mat'\n",
    "pathW        = path_proj + '02_src/02_SpecUFEx/out.DictGain.mat' \n",
    "\n",
    "\n",
    "# pathClusCat = path_proj + f\"principalDf_full_{mode}_Kopt{Kopt}.csv\"\n",
    "dataH5_path = path_proj + dataFile_name\n",
    "\n",
    "\n",
    "projNameN        = pN['projName']\n",
    "datasetIDN       = pN['datasetID']\n",
    "projNameN        = pN['projName']\n",
    "station         = pN['station']\n",
    "channel         = pN['channel']\n",
    "\n",
    "\n",
    "path_top        = pN['path_top']\n",
    "path_projN       = pN['path_proj']\n",
    "outfile_nameN    = pN['outfile_name']\n",
    "dataFile_nameN   = pN['dataFile_name']\n",
    "path_WFN         = pN['path_WF']\n",
    "path_CatN        = pN['path_Cat'] #original, raw catalog\n",
    "subCatalog_NameN = f\"{dataFile_name}_Sgrams_Subcatalog.hdf5\"\n",
    "\n",
    "\n",
    "pathACMN         = f'{path_top}{projNameN}/03_output/{station}/SpecUFEx_output/step2_NMF/'\n",
    "pathSTMN         = f'{path_top}{projNameN}/03_output/{station}/SpecUFEx_output/step4_stateTransMats/'\n",
    "pathEBN          = f'{path_top}{projNameN}/02_src/02_SpecUFEx/EB.mat'\n",
    "pathElnBN          = f'{path_top}{projNameN}/02_src/02_SpecUFEx/ElnB.mat'\n",
    "pathWN        = path_projN + '02_src/02_SpecUFEx/out.DictGain.mat' \n",
    "\n",
    "\n",
    "\n",
    "# pathClusCatN = path_projN + f\"principalDf_full_{mode}_Kopt{KoptN}.csv\"\n",
    "dataH5_pathN = path_projN + dataFile_nameN\n",
    "\n",
    "\n",
    "pathFig = '../reports/figures/'\n",
    "pathAuxData = '../data/external/GarciaEtAl_2019/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load auxiliary catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lake_D</th>\n",
       "      <th>temp_D</th>\n",
       "      <th>rain_D</th>\n",
       "      <th>gps24_D</th>\n",
       "      <th>gps34_D</th>\n",
       "      <th>gps36_D</th>\n",
       "      <th>gps37_D</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-06-14</th>\n",
       "      <td>25.738000</td>\n",
       "      <td>3.526000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2007-06-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-15</th>\n",
       "      <td>26.140615</td>\n",
       "      <td>7.312125</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>0.016641</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>2007-06-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   lake_D    temp_D    rain_D   gps24_D   gps34_D   gps36_D  \\\n",
       "datetime_index                                                                \n",
       "2007-06-14      25.738000  3.526000  0.500000  0.000000  0.000000  0.000000   \n",
       "2007-06-15      26.140615  7.312125  0.054167  0.033048  0.016641  0.010967   \n",
       "\n",
       "                 gps37_D   datetime  \n",
       "datetime_index                       \n",
       "2007-06-14      0.000000 2007-06-14  \n",
       "2007-06-15      0.012695 2007-06-15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garciaDF_H = pd.read_csv(f'{pathAuxData}garciaDF_H.csv',index_col=0)\n",
    "garciaDF_3H = pd.read_csv(f'{pathAuxData}garciaDF_3H.csv',index_col=0)\n",
    "garciaDF_D = pd.read_csv(f'{pathAuxData}garciaDF_D.csv',index_col=0)\n",
    "\n",
    "\n",
    "## when loading csv or text, sometimes need to reconvert this column to pandas datetime\n",
    "garciaDF_H['datetime'] = [pd.to_datetime(ii) for ii in garciaDF_H.index]\n",
    "garciaDF_3H['datetime'] = [pd.to_datetime(ii) for ii in garciaDF_3H.index]\n",
    "garciaDF_D['datetime'] = [pd.to_datetime(ii) for ii in garciaDF_D.index]\n",
    "\n",
    "garciaDF_H['datetime_index'] = [pd.to_datetime(ii) for ii in garciaDF_H.index]\n",
    "garciaDF_3H['datetime_index'] = [pd.to_datetime(ii) for ii in garciaDF_3H.index]\n",
    "garciaDF_D['datetime_index'] = [pd.to_datetime(ii) for ii in garciaDF_D.index]\n",
    "\n",
    "\n",
    "garciaDF_H = garciaDF_H.set_index('datetime_index')\n",
    "garciaDF_3H = garciaDF_3H.set_index('datetime_index')\n",
    "garciaDF_D = garciaDF_D.set_index('datetime_index')\n",
    "\n",
    "\n",
    "\n",
    "garciaDF_D.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some important times in study period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing of lake events \n",
    "tstart = dtt.datetime(2007, 6, 13)\n",
    "tend = dtt.datetime(2007, 7, 23)\n",
    "calvet = dtt.datetime(2007, 7, 1,13,41,35)\n",
    "supraDraint = dtt.datetime(2007, 7, 4)\n",
    "subDraint = dtt.datetime(2007, 7, 7)\n",
    "drainEndt = dtt.datetime(2007, 7, 15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load original catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411 events kept, merge sucessful\n",
      "1521 events kept, merge sucessful\n"
     ]
    }
   ],
   "source": [
    "cat00 = catMergeFromH5(path_Cat,path_proj,outfile_name)\n",
    "cat00N = catMergeFromH5(path_CatN,path_projN,outfile_nameN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##station data \n",
    "stn = pd.read_csv(\"../data/raw/stnlst.csv\",\n",
    "                  header=None,\n",
    "                  names=['name','X','Y','Elevation','dX','dY','Depth'])\n",
    "\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experiment parameters from H5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "####IQIQIQIQIQIQIQIQI\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(path_proj + dataFile_name,'r') as dataFile:\n",
    "\n",
    "    lenData = dataFile['processing_info/'].get('lenData')[()]\n",
    "    fs = dataFile['spec_parameters/'].get('fs')[()]\n",
    "    \n",
    "    # fmin = \n",
    "    nperseg = dataFile['spec_parameters/'].get('nperseg')[()]\n",
    "    noverlap = dataFile['spec_parameters/'].get('noverlap')[()]\n",
    "    nfft = dataFile['spec_parameters/'].get('nfft')[()]\n",
    "\n",
    "\n",
    "    fmax = dataFile['spec_parameters/'].get('fmax')[()]\n",
    "    fmax = np.ceil(fmax)\n",
    "    fmin = dataFile['spec_parameters/'].get('fmin')[()]\n",
    "    fmin = np.floor(fmin)    \n",
    "    fSTFT = dataFile['spec_parameters/'].get('fSTFT')[()]\n",
    "    tSTFT = dataFile['spec_parameters/'].get('tSTFT')[()]\n",
    "    \n",
    "    sgram_mode = dataFile['spec_parameters/'].get('mode')[()].decode('utf-8')\n",
    "    scaling = dataFile['spec_parameters/'].get('scaling')[()].decode('utf-8')\n",
    "    \n",
    "    \n",
    "fs = int(np.ceil(fs))\n",
    "winLen_Sec = float(nperseg / fs)\n",
    "\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "##### NOISENOISENOISENOISENOISE\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "\n",
    "with h5py.File(path_projN + dataFile_nameN,'r') as dataFile:\n",
    "\n",
    "    lenDataN = dataFile['processing_info/'].get('lenData')[()]\n",
    "    fsN = dataFile['spec_parameters/'].get('fs')[()]\n",
    "    \n",
    "    # fminN = \n",
    "    npersegN = dataFile['spec_parameters/'].get('nperseg')[()]\n",
    "    noverlapN = dataFile['spec_parameters/'].get('noverlap')[()]\n",
    "    nfftN = dataFile['spec_parameters/'].get('nfft')[()]\n",
    "\n",
    "\n",
    "    fmaxN = dataFile['spec_parameters/'].get('fmax')[()]\n",
    "    fmaxN = np.ceil(fmaxN)\n",
    "    fminN = dataFile['spec_parameters/'].get('fmin')[()]\n",
    "    fminN = np.floor(fminN)    \n",
    "    fSTFTN = dataFile['spec_parameters/'].get('fSTFT')[()]\n",
    "    tSTFTN = dataFile['spec_parameters/'].get('tSTFT')[()]\n",
    "    \n",
    "    sgram_modeN = dataFile['spec_parameters/'].get('mode')[()].decode('utf-8')\n",
    "    scalingN = dataFile['spec_parameters/'].get('scaling')[()].decode('utf-8')\n",
    "    \n",
    "    \n",
    "fsN = int(np.ceil(fsN))\n",
    "winLen_SecN = float(npersegN / fsN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load specufex output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "## specufex output - IQIQIQIQIQIQIQIQIQIQ\n",
    " \n",
    "#########       #########       #########       #########       #########       #########       #########       #########       Wmat = loadmat(pathW)\n",
    "\n",
    "Wmat = loadmat(pathW)\n",
    "EBmat = loadmat(pathEB)\n",
    "\n",
    "W = Wmat.get('W1')\n",
    "EB = EBmat.get('EB')\n",
    "\n",
    "\n",
    "\n",
    "numPatterns = len(W[1])\n",
    "Nfreqs = len(W)\n",
    "numStates = EB.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "order_swap = getNMFOrder(W,numPatterns)\n",
    "W_new = resortByNMF(W,order_swap)\n",
    "EB_new = resortByNMF(EB,order_swap)\n",
    "\n",
    "RMM = W_new @ EB_new.T\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "## specufex output - NOISENOISENOINSENOISE\n",
    "\n",
    "#########       #########       #########       #########       #########       #########       #########       #########       \n",
    "\n",
    "\n",
    "WmatN = loadmat(pathWN)\n",
    "EBmatN = loadmat(pathEBN)\n",
    "\n",
    "WN = WmatN.get('W1')\n",
    "EBN = EBmatN.get('EB')\n",
    "\n",
    "\n",
    "\n",
    "numPatternsN = len(WN[1])\n",
    "NfreqsN = len(WN)\n",
    "numStatesN = EBN.shape[0]\n",
    "\n",
    "\n",
    "order_swapN = getNMFOrder(WN,numPatternsN)\n",
    "W_newN = resortByNMF(WN,order_swapN)\n",
    "EB_newN = resortByNMF(EBN,order_swapN)\n",
    "\n",
    "RMMN = W_newN @ EB_newN.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format day ticks, time plotting\n",
    "\n",
    "* Central European Time is 2 hours later than UTC (Coordinated Universal Time) \n",
    "* Max temp occurs around 16:00 (4pm) local time or, 14:00 (2pm) UTC\n",
    "* All times in UTC\n",
    "\n",
    "\n",
    "todo: fix ::\n",
    "\n",
    "\n",
    "##dummy variable -- just needed to get complete day set -- FIXFIX\n",
    "clus_clu_perday = cat0.event_ID.resample('D', label='left', closed='right').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6-14', '6-21', '6-28', '7-5', '7-12', '7-19']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "            ##### FORMAT DAY TICKS (ASSUMES NO DAYS SKIPPED?) ######\n",
    "############################################################\n",
    "tstart = pd.to_datetime('2007-06-14 00:00:00')\n",
    "tend   = pd.to_datetime('2007-07-22 00:00:00')\n",
    "\n",
    "\n",
    "\n",
    "delta_day = 7\n",
    "\n",
    "##dummy variable -- just needed to get complete day set -- FIXFIX\n",
    "clus_clu_perday = cat00.event_ID.resample('D', label='left', closed='right').count()\n",
    "\n",
    "numDays = len(clus_clu_perday)\n",
    "\n",
    "days_list = [clus_clu_perday.index[i] for i in range(numDays)]\n",
    "\n",
    "\n",
    "## these have lots of possible text formats\n",
    "day_labels = [f\"{days_list[d].month}-{days_list[d].date().day}\" for d in range(0,len(days_list),delta_day)]\n",
    "\n",
    "day_ticks = [days_list[d] for d in range(0,len(days_list),delta_day)]\n",
    "\n",
    "\n",
    "# Central European Time is 2 hours later than UTC (Coordinated Universal Time)\n",
    "##max temp is around 4pm local time or 16:00, in UTC it is 14:00 or 2pm\n",
    "#all times in UTC\n",
    "hour_of_approx_max_temp = 14\n",
    "hourMaxTemp = [dtt.datetime(2007, 6, 14,hour_of_approx_max_temp,0,0) + pd.DateOffset(i) for i in range(0,numDays)]\n",
    "\n",
    "hour24labels = [str(r) for r in range(0,24)] #UTC\n",
    "\n",
    "print(day_labels)\n",
    "############################################################\n",
    "############################################################\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap']='magma'\n",
    "\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "colors =cm.Paired(np.array([1,5,7,9,2,4,6,8]))\n",
    "\n",
    "\n",
    "## when plotting, add a bit of buffer so bars aren't cut off\n",
    "tlimstart = pd.to_datetime('2007-06-13 12:00:00')\n",
    "tlimend   = pd.to_datetime('2007-07-22 12:00:00')\n",
    "\n",
    "\n",
    "lw1=4        \n",
    "lw2=5\n",
    "alphaT=1\n",
    "ylabfont=8\n",
    "ylabpad =10\n",
    "\n",
    "\n",
    "plt_kwargs = {'lw1':lw1,\n",
    "              'lw2':lw2,\n",
    "              'alphaT':alphaT,\n",
    "              'ylabfont':ylabfont,\n",
    "              'ylabpad':ylabpad,\n",
    "              'colors':colors,\n",
    "              'scaling':scaling,\n",
    "              'sgram_mode':sgram_mode,\n",
    "              'hour24labels':hour24labels,\n",
    "              'day_ticks':day_ticks,\n",
    "              'day_labels':day_labels,\n",
    "              'numDays':numDays,\n",
    "              'hourMaxTemp':hourMaxTemp,\n",
    "              'tstart':tlimstart, ## for extending x axis to fit bars\n",
    "              'tend':tlimend,     ## for extending x axis to fit bars\n",
    "              'tstartreal':tstart,## actual study bound\n",
    "              'tendreal':tend     ## actual study bound\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specs for figures JGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quarter page\n",
    "width1 = 3.74016\n",
    "height1 = 4.52756\n",
    "\n",
    "#full page\n",
    "width2 = 7.48031\n",
    "height2 = 9.05512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 5 - Reduce dimensionality, visualize PC plots by time, hour\n",
    "\n",
    "* Perform PCA on FPs \n",
    "* Calculate PVE (percent variance explained) for each PC, and keep enouch PCs to capture XX% of variance\n",
    "* Cluster using Kmeans \n",
    "* Choose cluster based on highest mean silhouette score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PCA_df, numPCA_PVE, cum_pve = PVEofPCA(path_proj,outfile_name,cat00,numPCMax=numStates**2,cum_pve_thresh=.86,stand=False);\n",
    "PCA_dfN, numPCA_PVEN, cum_pveN = PVEofPCA(path_projN,outfile_nameN,cat00N,numPCMax=numStates**2,cum_pve_thresh=.86,stand=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " For icequakes, 86.31% of variance explained by the first 67 principal components\n",
      "\n",
      " For noise, 86.09% of variance explained by the first 3 principal components\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n For icequakes, {100*cum_pve:.2f}% of variance explained by the first {numPCA_PVE} principal components')\n",
    "print(f'\\n For noise, {100*cum_pveN:.2f}% of variance explained by the first {numPCA_PVEN} principal components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand = False #standard scalar before PCA\n",
    "\n",
    "\n",
    "#returns a PCA sklearn object, a dataframe of cat00 but with columns for PCs, and a numpy array of PCs (N x numPC)\n",
    "\n",
    "sklearn_pca, cat00, Y_PCA = PCAonFP(path_proj,outfile_name,cat00,numPCA=numPCA_PVE,stand=stand)\n",
    "sklearn_pcaN, cat00N, Y_PCAN = PCAonFP(path_projN,outfile_nameN,cat00N,numPCA=numPCA_PVEN,stand=stand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using Kmeans, validating using silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans on 2 clusters...\n",
      "kmeans on 3 clusters...\n",
      "kmeans on 4 clusters...\n",
      "kmeans on 5 clusters...\n",
      "kmeans on 6 clusters...\n",
      "kmeans on 7 clusters...\n",
      "kmeans on 8 clusters...\n",
      "kmeans on 9 clusters...\n",
      "kmeans on 10 clusters...\n",
      "Best cluster: 3\n",
      "Event optimum number of clusters::  3\n",
      "kmeans on 2 clusters...\n",
      "kmeans on 3 clusters...\n",
      "kmeans on 4 clusters...\n",
      "kmeans on 5 clusters...\n",
      "kmeans on 6 clusters...\n",
      "kmeans on 7 clusters...\n",
      "kmeans on 8 clusters...\n",
      "kmeans on 9 clusters...\n",
      "kmeans on 10 clusters...\n",
      "Best cluster: 4\n",
      "Noise optimum number of clusters::  4\n"
     ]
    }
   ],
   "source": [
    "## silh score and validation\n",
    "##making rep catalog\n",
    "\n",
    "\n",
    "##can force K here\n",
    "Kmax=10\n",
    "range_n_clusters_all = list(range(2,Kmax+1))\n",
    "\n",
    "\n",
    "cat00, catall, Kopt, maxSilScore, avgSils, sse,cluster_labels_best,ss_best,euc_dist_best = calcSilhScore(path_proj,outfile_name,cat00,range_n_clusters_all,numPCA=numPCA_PVE,Xtype='fingerprints', distMeasure = \"SilhScore\",stand=stand);\n",
    "\n",
    "print('Event optimum number of clusters:: ',Kopt)\n",
    "\n",
    "\n",
    "cat00N, catallN, KoptN, maxSilScoreN, avgSilsN, sseN,cluster_labels_bestN,ss_bestN,euc_dist_bestN = calcSilhScore(path_projN,outfile_nameN,cat00N,range_n_clusters_all,numPCA=numPCA_PVEN,Xtype='fingerprints',distMeasure = \"SilhScore\", stand=stand);\n",
    "\n",
    "\n",
    "print('Noise optimum number of clusters:: ',KoptN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 most representative events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "catRep = getTopFCat(cat00,topF=10) \n",
    "\n",
    "catRepN = getTopFCat(cat00N,topF=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## move to analysis\n",
    "# clus_sel = [1,2,3]\n",
    "# sel_state = [12,14,8]\n",
    "\n",
    "# clus_selN = [1,2,3,4]\n",
    "# sel_stateN = [6,3,5,4]\n",
    "\n",
    "# leg = ['During/after flood','Before flood','PM']\n",
    "# legN = ['During/after flood','Before flood','Rain?','PM']\n",
    "\n",
    "# ## move to clustering\n",
    "# mode = 'fingerprints'#'kernalPCA'#'fingerprints'#'PCA' \n",
    "\n",
    "# print(key,mode,Kopt, ' clusters')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
